{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "745efcec",
   "metadata": {},
   "source": [
    "# Pré-processamento e Transformação de Dados\n",
    "\n",
    "## Objetivos\n",
    "\n",
    "- Compreender a importância do pré-processamento\n",
    "- Implementar transformações numéricas e categóricas\n",
    "- Usar StandardScaler, MinMaxScaler, RobustScaler\n",
    "- Aplicar OneHotEncoder e LabelEncoder\n",
    "- Trabalhar com dados faltantes\n",
    "\n",
    "## Pré-requisitos\n",
    "\n",
    "- Manipulação de dados com pandas\n",
    "- Conceitos básicos de ML\n",
    "- Pipelines em sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d3fb19",
   "metadata": {},
   "source": [
    "## 1. Por que Pré-processamento?\n",
    "\n",
    "Diferentes algoritmos têm diferentes requisitos:\n",
    "\n",
    "- **Árvores de decisão**: Não precisam de normalização\n",
    "- **SVM, KNN**: Sensíveis à escala das features\n",
    "- **Redes neurais**: Requerem normalização para convergência\n",
    "- **Linear models**: Beneficiam de normalização\n",
    "\n",
    "Problemas comuns:\n",
    "\n",
    "- Features em escalas diferentes\n",
    "- Variáveis categóricas não numéricas\n",
    "- Dados faltantes\n",
    "- Outliers extremos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbfd968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    MinMaxScaler,\n",
    "    RobustScaler,\n",
    "    OneHotEncoder,\n",
    "    LabelEncoder,\n",
    "    OrdinalEncoder,\n",
    ")\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configurar visualização\n",
    "plt.style.use(\"default\")\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a195601e",
   "metadata": {},
   "source": [
    "## 2. Demonstrando o Problema da Escala\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8904fb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar dataset com features em escalas diferentes\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Features em escalas muito diferentes\n",
    "age = np.random.normal(35, 10, n_samples)  # Idade: 15-55\n",
    "income = np.random.normal(50000, 15000, n_samples)  # Renda: 20k-80k\n",
    "score = np.random.normal(0.7, 0.1, n_samples)  # Score: 0.4-1.0\n",
    "\n",
    "# Target baseado em combinação das features\n",
    "y = (\n",
    "    0.3 * (age - 35) / 10\n",
    "    + 0.4 * (income - 50000) / 15000\n",
    "    + 0.3 * (score - 0.7) / 0.1\n",
    "    + np.random.normal(0, 0.1, n_samples)\n",
    ") > 0\n",
    "\n",
    "# Criar DataFrame\n",
    "df = pd.DataFrame({\"age\": age, \"income\": income, \"score\": score, \"target\": y.astype(int)})\n",
    "\n",
    "print(\"Estatísticas descritivas:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Visualizar diferenças de escala\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "for i, col in enumerate([\"age\", \"income\", \"score\"]):\n",
    "    axes[i].hist(df[col], bins=30, alpha=0.7, edgecolor=\"black\")\n",
    "    axes[i].set_title(f\"{col.title()}\\nRange: {df[col].min():.0f} - {df[col].max():.0f}\")\n",
    "    axes[i].set_ylabel(\"Frequência\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c510a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrar impacto nos algoritmos\n",
    "X = df[[\"age\", \"income\", \"score\"]]\n",
    "y = df[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Testar modelos sem normalização\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"SVM\": SVC(random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42),\n",
    "}\n",
    "\n",
    "print(\"=== Performance SEM normalização ===\")\n",
    "scores_without = {}\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "    scores_without[name] = scores\n",
    "    print(f\"{name:20}: {scores.mean():.3f} ± {scores.std():.3f}\")\n",
    "\n",
    "# Testar modelos COM normalização\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\n=== Performance COM normalização ===\")\n",
    "scores_with = {}\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X_train_scaled, y_train, cv=5)\n",
    "    scores_with[name] = scores\n",
    "    print(f\"{name:20}: {scores.mean():.3f} ± {scores.std():.3f}\")\n",
    "\n",
    "# Visualizar comparação\n",
    "improvement = {}\n",
    "for name in models.keys():\n",
    "    improvement[name] = scores_with[name].mean() - scores_without[name].mean()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(improvement.keys(), improvement.values())\n",
    "plt.axhline(y=0, color=\"black\", linestyle=\"--\", alpha=0.5)\n",
    "plt.ylabel(\"Melhoria na Accuracy\")\n",
    "plt.title(\"Impacto da Normalização nos Modelos\")\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Colorir barras\n",
    "for i, bar in enumerate(bars):\n",
    "    if improvement[list(improvement.keys())[i]] > 0:\n",
    "        bar.set_color(\"green\")\n",
    "    else:\n",
    "        bar.set_color(\"red\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdc8426",
   "metadata": {},
   "source": [
    "## 3. Tipos de Scalers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54519d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar dados com outliers para demonstrar diferentes scalers\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "normal_data = np.random.normal(10, 2, n_samples)\n",
    "\n",
    "# Adicionar outliers\n",
    "outliers = np.random.choice(n_samples, size=20, replace=False)\n",
    "normal_data[outliers] += np.random.normal(0, 10, 20)\n",
    "\n",
    "data = normal_data.reshape(-1, 1)\n",
    "\n",
    "# Aplicar diferentes scalers\n",
    "scalers = {\n",
    "    \"Original\": None,\n",
    "    \"StandardScaler\": StandardScaler(),\n",
    "    \"MinMaxScaler\": MinMaxScaler(),\n",
    "    \"RobustScaler\": RobustScaler(),\n",
    "}\n",
    "\n",
    "scaled_data = {}\n",
    "for name, scaler in scalers.items():\n",
    "    if scaler is None:\n",
    "        scaled_data[name] = data.flatten()\n",
    "    else:\n",
    "        scaled_data[name] = scaler.fit_transform(data).flatten()\n",
    "\n",
    "# Visualizar resultados\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (name, data_scaled) in enumerate(scaled_data.items()):\n",
    "    axes[i].hist(data_scaled, bins=30, alpha=0.7, edgecolor=\"black\")\n",
    "    axes[i].set_title(f\"{name}\\nMédia: {np.mean(data_scaled):.2f}, Std: {np.std(data_scaled):.2f}\")\n",
    "    axes[i].axvline(np.mean(data_scaled), color=\"red\", linestyle=\"--\", label=\"Média\")\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Mostrar fórmulas\n",
    "print(\"Fórmulas dos Scalers:\")\n",
    "print(\"StandardScaler: (x - μ) / σ\")\n",
    "print(\"MinMaxScaler: (x - min) / (max - min)\")\n",
    "print(\"RobustScaler: (x - median) / IQR\")\n",
    "print(\"\\nQuando usar cada um:\")\n",
    "print(\"- StandardScaler: Distribuição normal, poucos outliers\")\n",
    "print(\"- MinMaxScaler: Range específico [0,1], distribuição uniforme\")\n",
    "print(\"- RobustScaler: Muitos outliers, distribuições assimétricas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe1ba52",
   "metadata": {},
   "source": [
    "## 4. Tratamento de Variáveis Categóricas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9add49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar dataset com variáveis categóricas\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Variáveis categóricas\n",
    "cities = np.random.choice([\"São Paulo\", \"Rio de Janeiro\", \"Belo Horizonte\", \"Salvador\"], n_samples)\n",
    "education = np.random.choice([\"Fundamental\", \"Médio\", \"Superior\", \"Pós-graduação\"], n_samples)\n",
    "size = np.random.choice([\"Pequeno\", \"Médio\", \"Grande\"], n_samples)\n",
    "\n",
    "# Variável numérica\n",
    "income = np.random.normal(5000, 1500, n_samples)\n",
    "\n",
    "# Target sintético\n",
    "city_effect = {\"São Paulo\": 0.3, \"Rio de Janeiro\": 0.2, \"Belo Horizonte\": 0.1, \"Salvador\": 0}\n",
    "edu_effect = {\"Fundamental\": 0, \"Médio\": 0.1, \"Superior\": 0.3, \"Pós-graduação\": 0.5}\n",
    "size_effect = {\"Pequeno\": 0, \"Médio\": 0.2, \"Grande\": 0.4}\n",
    "\n",
    "y_prob = (\n",
    "    np.array([city_effect[c] for c in cities])\n",
    "    + np.array([edu_effect[e] for e in education])\n",
    "    + np.array([size_effect[s] for s in size])\n",
    "    + (income - 5000) / 10000\n",
    ")\n",
    "\n",
    "y = (y_prob + np.random.normal(0, 0.1, n_samples)) > 0.5\n",
    "\n",
    "# Criar DataFrame\n",
    "df_cat = pd.DataFrame(\n",
    "    {\"city\": cities, \"education\": education, \"company_size\": size, \"income\": income, \"target\": y.astype(int)}\n",
    ")\n",
    "\n",
    "print(\"Dataset com variáveis categóricas:\")\n",
    "print(df_cat.head())\n",
    "print(\"\\nTipos de dados:\")\n",
    "print(df_cat.dtypes)\n",
    "print(\"\\nValores únicos:\")\n",
    "for col in [\"city\", \"education\", \"company_size\"]:\n",
    "    print(f\"{col}: {df_cat[col].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5a6650",
   "metadata": {},
   "source": [
    "### 4.1 One-Hot Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e36f2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding para variáveis nominais\n",
    "print(\"=== One-Hot Encoding ===\")\n",
    "\n",
    "# Método 1: pandas get_dummies\n",
    "df_dummies = pd.get_dummies(df_cat, columns=[\"city\", \"education\"], prefix=[\"city\", \"edu\"])\n",
    "print(f\"Colunas após get_dummies: {df_dummies.shape[1]}\")\n",
    "print(\"Primeiras colunas:\", df_dummies.columns[:10].tolist())\n",
    "\n",
    "# Método 2: sklearn OneHotEncoder\n",
    "encoder = OneHotEncoder(drop=\"first\", sparse_output=False)  # drop='first' evita multicolinearidade\n",
    "city_encoded = encoder.fit_transform(df_cat[[\"city\", \"education\"]])\n",
    "feature_names = encoder.get_feature_names_out([\"city\", \"education\"])\n",
    "\n",
    "print(f\"\\nShape após OneHotEncoder: {city_encoded.shape}\")\n",
    "print(f\"Nomes das features: {feature_names}\")\n",
    "\n",
    "# Demonstrar problema da \"curse of dimensionality\"\n",
    "print(f\"\\nDimensionalidade:\")\n",
    "print(f\"Original: {df_cat.shape[1]} features\")\n",
    "print(f\"Após One-Hot: {len(feature_names) + 2} features\")  # +2 para income e company_size\n",
    "\n",
    "# Visualizar encoding de uma variável\n",
    "sample_cities = df_cat[\"city\"].head(10)\n",
    "sample_encoded = encoder.transform(df_cat[[\"city\", \"education\"]].head(10))\n",
    "\n",
    "print(\"\\nExemplo de encoding:\")\n",
    "for i in range(5):\n",
    "    print(f\"{sample_cities.iloc[i]} -> {sample_encoded[i][:4]}...\")  # Mostrar só primeiras 4 colunas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea835f9",
   "metadata": {},
   "source": [
    "### 4.2 Label Encoding e Ordinal Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfa422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding (cuidado com variáveis nominais!)\n",
    "print(\"=== Label Encoding ===\")\n",
    "\n",
    "le_city = LabelEncoder()\n",
    "city_labels = le_city.fit_transform(df_cat[\"city\"])\n",
    "\n",
    "print(\"Mapeamento cidade -> número:\")\n",
    "for i, city in enumerate(le_city.classes_):\n",
    "    print(f\"{city} -> {i}\")\n",
    "\n",
    "print(f\"\\nExemplos: {df_cat['city'].head().tolist()} -> {city_labels[:5].tolist()}\")\n",
    "\n",
    "# Ordinal Encoding (para variáveis ordinais)\n",
    "print(\"\\n=== Ordinal Encoding ===\")\n",
    "\n",
    "# Education tem ordem natural\n",
    "education_order = [[\"Fundamental\", \"Médio\", \"Superior\", \"Pós-graduação\"]]\n",
    "ordinal_encoder = OrdinalEncoder(categories=education_order)\n",
    "education_ordinal = ordinal_encoder.fit_transform(df_cat[[\"education\"]])\n",
    "\n",
    "print(\"Mapeamento educação (ordinal):\")\n",
    "for i, edu in enumerate(education_order[0]):\n",
    "    print(f\"{edu} -> {i}\")\n",
    "\n",
    "# Company size também tem ordem\n",
    "size_order = [[\"Pequeno\", \"Médio\", \"Grande\"]]\n",
    "size_encoder = OrdinalEncoder(categories=size_order)\n",
    "size_ordinal = size_encoder.fit_transform(df_cat[[\"company_size\"]])\n",
    "\n",
    "print(\"\\nMapeamento tamanho empresa:\")\n",
    "for i, size in enumerate(size_order[0]):\n",
    "    print(f\"{size} -> {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9cb2d7",
   "metadata": {},
   "source": [
    "## 5. Tratamento de Dados Faltantes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e72bc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar dataset com valores faltantes\n",
    "np.random.seed(42)\n",
    "n_samples = 500\n",
    "\n",
    "# Dataset base\n",
    "age = np.random.normal(35, 10, n_samples)\n",
    "income = np.random.normal(50000, 15000, n_samples)\n",
    "score = np.random.normal(0.7, 0.1, n_samples)\n",
    "category = np.random.choice([\"A\", \"B\", \"C\"], n_samples)\n",
    "\n",
    "# Introduzir valores faltantes de forma realística\n",
    "# Missing at Random (MAR) - probabilidade depende de outras variáveis\n",
    "missing_prob_age = 0.1 + 0.1 * (income < 30000)  # Mais missings em renda baixa\n",
    "missing_prob_income = 0.05 + 0.15 * (age > 50)  # Mais missings em idade alta\n",
    "missing_prob_score = 0.08  # Missing Completely at Random (MCAR)\n",
    "\n",
    "age[np.random.random(n_samples) < missing_prob_age] = np.nan\n",
    "income[np.random.random(n_samples) < missing_prob_income] = np.nan\n",
    "score[np.random.random(n_samples) < missing_prob_score] = np.nan\n",
    "\n",
    "# Missing em categoria (Not Missing at Random - NMAR)\n",
    "category[np.random.choice(n_samples, size=30, replace=False)] = None\n",
    "\n",
    "df_missing = pd.DataFrame({\"age\": age, \"income\": income, \"score\": score, \"category\": category})\n",
    "\n",
    "print(\"Dataset com valores faltantes:\")\n",
    "print(df_missing.info())\n",
    "print(\"\\nPercentual de missings por coluna:\")\n",
    "missing_pct = (df_missing.isnull().sum() / len(df_missing)) * 100\n",
    "print(missing_pct)\n",
    "\n",
    "# Visualizar padrão de missings\n",
    "import missingno as msno\n",
    "\n",
    "try:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    # Heatmap de correlação de missings\n",
    "    missing_df = df_missing.isnull()\n",
    "    correlation_missing = missing_df.corr()\n",
    "\n",
    "    sns.heatmap(correlation_missing, annot=True, cmap=\"coolwarm\", center=0, ax=axes[0])\n",
    "    axes[0].set_title(\"Correlação entre Padrões de Missing\")\n",
    "\n",
    "    # Contagem de missings por linha\n",
    "    missing_counts = missing_df.sum(axis=1)\n",
    "    axes[1].hist(missing_counts, bins=5, edgecolor=\"black\")\n",
    "    axes[1].set_xlabel(\"Número de features faltantes por amostra\")\n",
    "    axes[1].set_ylabel(\"Frequência\")\n",
    "    axes[1].set_title(\"Distribuição de Missings por Linha\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "except ImportError:\n",
    "    print(\"Biblioteca missingno não instalada. Criando visualização alternativa...\")\n",
    "\n",
    "    # Visualização alternativa\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    missing_matrix = df_missing.isnull()\n",
    "    plt.imshow(missing_matrix.T, cmap=\"RdYlBu\", aspect=\"auto\")\n",
    "    plt.yticks(range(len(df_missing.columns)), df_missing.columns)\n",
    "    plt.xlabel(\"Amostras\")\n",
    "    plt.title(\"Padrão de Valores Faltantes\\n(Azul = Presente, Vermelho = Faltante)\")\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a466f1",
   "metadata": {},
   "source": [
    "### 5.1 Estratégias de Imputação\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a31cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diferentes estratégias de imputação\n",
    "print(\"=== Estratégias de Imputação ===\")\n",
    "\n",
    "# 1. Simple Imputer - estratégias básicas\n",
    "imputers = {\n",
    "    \"Mean\": SimpleImputer(strategy=\"mean\"),\n",
    "    \"Median\": SimpleImputer(strategy=\"median\"),\n",
    "    \"Most Frequent\": SimpleImputer(strategy=\"most_frequent\"),\n",
    "    \"Constant\": SimpleImputer(strategy=\"constant\", fill_value=0),\n",
    "}\n",
    "\n",
    "# Testar apenas em colunas numéricas\n",
    "numeric_cols = [\"age\", \"income\", \"score\"]\n",
    "X_missing = df_missing[numeric_cols]\n",
    "\n",
    "imputed_results = {}\n",
    "for name, imputer in imputers.items():\n",
    "    if name == \"Most Frequent\" and any(df_missing[numeric_cols].dtypes == \"object\"):\n",
    "        continue  # Pular se não houver categóricas\n",
    "\n",
    "    X_imputed = imputer.fit_transform(X_missing)\n",
    "    imputed_results[name] = X_imputed\n",
    "\n",
    "    # Calcular estatísticas\n",
    "    print(f\"\\n{name}:\")\n",
    "    for i, col in enumerate(numeric_cols):\n",
    "        original_mean = df_missing[col].mean()\n",
    "        imputed_mean = X_imputed[:, i].mean()\n",
    "        print(f\"  {col}: {original_mean:.2f} -> {imputed_mean:.2f}\")\n",
    "\n",
    "# 2. KNN Imputer - imputação baseada em vizinhos\n",
    "print(\"\\n=== KNN Imputer ===\")\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "X_knn_imputed = knn_imputer.fit_transform(X_missing)\n",
    "\n",
    "print(\"KNN (k=5):\")\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    original_mean = df_missing[col].mean()\n",
    "    knn_mean = X_knn_imputed[:, i].mean()\n",
    "    print(f\"  {col}: {original_mean:.2f} -> {knn_mean:.2f}\")\n",
    "\n",
    "# Visualizar comparação das estratégias\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "strategies = [\"Mean\", \"Median\", \"Constant\"]\n",
    "strategies.append(\"KNN\")\n",
    "\n",
    "for i, strategy in enumerate(strategies[:4]):\n",
    "    if strategy == \"KNN\":\n",
    "        data_to_plot = X_knn_imputed[:, 0]  # Age column\n",
    "    else:\n",
    "        data_to_plot = imputed_results[strategy][:, 0]  # Age column\n",
    "\n",
    "    axes[i].hist(data_to_plot, bins=30, alpha=0.7, edgecolor=\"black\")\n",
    "    axes[i].set_title(f\"Age - {strategy} Imputation\")\n",
    "    axes[i].axvline(np.mean(data_to_plot), color=\"red\", linestyle=\"--\", label=\"Média\")\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d030430a",
   "metadata": {},
   "source": [
    "## 6. ColumnTransformer - Pré-processamento Completo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d835d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar dataset complexo que precisa de múltiplos tipos de pré-processamento\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Features numéricas com escalas diferentes\n",
    "age = np.random.normal(35, 10, n_samples)\n",
    "salary = np.random.normal(50000, 20000, n_samples)\n",
    "experience = np.random.normal(8, 5, n_samples)\n",
    "\n",
    "# Features categóricas\n",
    "department = np.random.choice([\"IT\", \"Marketing\", \"Sales\", \"HR\"], n_samples)\n",
    "education = np.random.choice([\"Bachelor\", \"Master\", \"PhD\"], n_samples)\n",
    "city = np.random.choice([\"SP\", \"RJ\", \"BH\"], n_samples)\n",
    "\n",
    "# Introduzir alguns valores faltantes\n",
    "age[np.random.choice(n_samples, 50, replace=False)] = np.nan\n",
    "salary[np.random.choice(n_samples, 30, replace=False)] = np.nan\n",
    "\n",
    "# Target\n",
    "y = (\n",
    "    0.3 * (age - 35) / 10\n",
    "    + 0.4 * (salary - 50000) / 20000\n",
    "    + 0.3 * (experience - 8) / 5\n",
    "    + np.random.normal(0, 0.5, n_samples)\n",
    ") > 0\n",
    "\n",
    "df_complex = pd.DataFrame(\n",
    "    {\n",
    "        \"age\": age,\n",
    "        \"salary\": salary,\n",
    "        \"experience\": experience,\n",
    "        \"department\": department,\n",
    "        \"education\": education,\n",
    "        \"city\": city,\n",
    "        \"target\": y.astype(int),\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Dataset complexo:\")\n",
    "print(df_complex.info())\n",
    "print(\"\\nPrimeiras linhas:\")\n",
    "print(df_complex.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82949337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir transformações específicas para cada tipo de coluna\n",
    "print(\"=== ColumnTransformer Completo ===\")\n",
    "\n",
    "# Separar features por tipo\n",
    "numeric_features = [\"age\", \"salary\", \"experience\"]\n",
    "categorical_features = [\"department\", \"education\", \"city\"]\n",
    "\n",
    "# Pipeline para features numéricas\n",
    "numeric_transformer = Pipeline(steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())])\n",
    "\n",
    "# Pipeline para features categóricas\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Combinar tudo com ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ],\n",
    "    remainder=\"drop\",  # Dropar colunas não especificadas\n",
    ")\n",
    "\n",
    "# Dividir dados\n",
    "X = df_complex.drop(\"target\", axis=1)\n",
    "y = df_complex[\"target\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Aplicar transformações\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"Shape original: {X_train.shape}\")\n",
    "print(f\"Shape após transformação: {X_train_transformed.shape}\")\n",
    "\n",
    "# Obter nomes das features transformadas\n",
    "feature_names = numeric_features + list(\n",
    "    preprocessor.named_transformers_[\"cat\"][\"onehot\"].get_feature_names_out(categorical_features)\n",
    ")\n",
    "\n",
    "print(f\"\\nFeatures finais ({len(feature_names)}):\")\n",
    "for i, name in enumerate(feature_names):\n",
    "    print(f\"{i+1:2d}. {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b80905",
   "metadata": {},
   "source": [
    "## 7. Pipeline Completo de ML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6c180c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar pipeline completo: pré-processamento + modelo\n",
    "print(\"=== Pipeline Completo de ML ===\")\n",
    "\n",
    "# Pipeline com Random Forest\n",
    "rf_pipeline = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"classifier\", RandomForestClassifier(random_state=42))])\n",
    "\n",
    "# Pipeline com SVM\n",
    "svm_pipeline = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"classifier\", SVC(random_state=42))])\n",
    "\n",
    "# Avaliar pipelines\n",
    "pipelines = {\"Random Forest\": rf_pipeline, \"SVM\": svm_pipeline}\n",
    "\n",
    "for name, pipeline in pipelines.items():\n",
    "    scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring=\"accuracy\")\n",
    "    print(f\"{name:15}: {scores.mean():.3f} ± {scores.std():.3f}\")\n",
    "\n",
    "# Treinar melhor modelo e avaliar\n",
    "best_pipeline = rf_pipeline\n",
    "best_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Avaliar no conjunto de teste\n",
    "train_score = best_pipeline.score(X_train, y_train)\n",
    "test_score = best_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(f\"\\nMelhor modelo (Random Forest):\")\n",
    "print(f\"Score treino: {train_score:.3f}\")\n",
    "print(f\"Score teste:  {test_score:.3f}\")\n",
    "print(f\"Diferença:    {train_score - test_score:.3f}\")\n",
    "\n",
    "# Mostrar importância das features\n",
    "feature_importance = best_pipeline.named_steps[\"classifier\"].feature_importances_\n",
    "importance_df = pd.DataFrame({\"feature\": feature_names, \"importance\": feature_importance}).sort_values(\n",
    "    \"importance\", ascending=False\n",
    ")\n",
    "\n",
    "print(\"\\nTop 10 features mais importantes:\")\n",
    "print(importance_df.head(10))\n",
    "\n",
    "# Visualizar importância\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_features = importance_df.head(10)\n",
    "plt.barh(range(len(top_features)), top_features[\"importance\"])\n",
    "plt.yticks(range(len(top_features)), top_features[\"feature\"])\n",
    "plt.xlabel(\"Importância\")\n",
    "plt.title(\"Top 10 Features Mais Importantes\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bfd42c",
   "metadata": {},
   "source": [
    "## 8. Validação do Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09ede80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testar pipeline com novos dados\n",
    "print(\"=== Validação do Pipeline ===\")\n",
    "\n",
    "# Criar novos dados de exemplo\n",
    "new_data = pd.DataFrame(\n",
    "    {\n",
    "        \"age\": [28, 45, np.nan, 35],\n",
    "        \"salary\": [45000, np.nan, 70000, 55000],\n",
    "        \"experience\": [3, 15, 8, 7],\n",
    "        \"department\": [\"IT\", \"Marketing\", \"Sales\", \"HR\"],\n",
    "        \"education\": [\"Bachelor\", \"Master\", \"PhD\", \"Bachelor\"],\n",
    "        \"city\": [\"SP\", \"RJ\", \"BH\", \"SP\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Novos dados (com missings):\")\n",
    "print(new_data)\n",
    "\n",
    "# Fazer predições\n",
    "predictions = best_pipeline.predict(new_data)\n",
    "probabilities = best_pipeline.predict_proba(new_data)\n",
    "\n",
    "print(\"\\nPredições:\")\n",
    "for i in range(len(new_data)):\n",
    "    print(f\"Amostra {i+1}: Classe {predictions[i]} (prob: {probabilities[i][1]:.3f})\")\n",
    "\n",
    "# Verificar se pipeline funciona com dados \"sujos\"\n",
    "dirty_data = pd.DataFrame(\n",
    "    {\n",
    "        \"age\": [25, 60, -5, 200],  # Idades irreais\n",
    "        \"salary\": [30000, 150000, -1000, 1000000],  # Salários extremos\n",
    "        \"experience\": [2, 40, -2, 100],  # Experiência irreal\n",
    "        \"department\": [\"IT\", \"Finance\", \"Unknown\", \"IT\"],  # Departamento novo\n",
    "        \"education\": [\"Bachelor\", \"Master\", \"High School\", \"Bachelor\"],  # Educação nova\n",
    "        \"city\": [\"SP\", \"RJ\", \"NYC\", \"SP\"],  # Cidade nova\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\n=== Teste com Dados 'Sujos' ===\")\n",
    "print(dirty_data)\n",
    "\n",
    "try:\n",
    "    dirty_predictions = best_pipeline.predict(dirty_data)\n",
    "    print(\"\\nPredições para dados sujos:\")\n",
    "    print(dirty_predictions)\n",
    "    print(\"✓ Pipeline lidou bem com dados problemáticos!\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Erro ao processar dados sujos: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c9c0b1",
   "metadata": {},
   "source": [
    "## 9. Resumo e Boas Práticas\n",
    "\n",
    "### Checklist de Pré-processamento:\n",
    "\n",
    "1. **Análise Exploratória**\n",
    "\n",
    "   - Tipos de dados\n",
    "   - Valores faltantes\n",
    "   - Distribuições\n",
    "   - Outliers\n",
    "\n",
    "2. **Features Numéricas**\n",
    "\n",
    "   - StandardScaler: distribuição normal\n",
    "   - MinMaxScaler: range específico\n",
    "   - RobustScaler: muitos outliers\n",
    "\n",
    "3. **Features Categóricas**\n",
    "\n",
    "   - OneHotEncoder: variáveis nominais\n",
    "   - OrdinalEncoder: variáveis ordinais\n",
    "   - Cuidado com alta cardinalidade\n",
    "\n",
    "4. **Dados Faltantes**\n",
    "\n",
    "   - Entender o mecanismo (MCAR, MAR, NMAR)\n",
    "   - SimpleImputer: estratégias básicas\n",
    "   - KNNImputer: valores baseados em similaridade\n",
    "\n",
    "5. **Pipeline**\n",
    "   - Sempre usar pipelines para evitar data leakage\n",
    "   - ColumnTransformer para diferentes tipos de features\n",
    "   - Testar com dados novos/sujos\n",
    "\n",
    "### Cuidados Importantes:\n",
    "\n",
    "- **Nunca** fazer fit do scaler em todo o dataset\n",
    "- **Sempre** aplicar mesmas transformações em treino/validação/teste\n",
    "- Tratar dados faltantes **antes** de dividir treino/teste\n",
    "- Validar pipeline com dados \"reais\" e problemáticos\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
