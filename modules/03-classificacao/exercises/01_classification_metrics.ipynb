{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54385183",
   "metadata": {},
   "source": [
    "# Exercício: Implementando Métricas de Classificação\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "Implementar métricas fundamentais de avaliação para modelos de classificação.\n",
    "\n",
    "## Instruções\n",
    "\n",
    "- Complete as funções nas células marcadas com `# TODO`\n",
    "- Mantenha a assinatura das funções\n",
    "- Use apenas numpy e pandas\n",
    "- Execute as células de teste para verificar sua implementação\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcfe042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importações permitidas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Configurar seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ff5923",
   "metadata": {},
   "source": [
    "## Exercício 1: Acurácia (Accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2cfa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calcula a acurácia entre valores reais e preditos.\n",
    "\n",
    "    Parâmetros:\n",
    "    y_true (array-like): Valores reais\n",
    "    y_pred (array-like): Valores preditos\n",
    "\n",
    "    Retorna:\n",
    "    float: Acurácia calculada (entre 0 e 1)\n",
    "    \"\"\"\n",
    "    # TODO: Implementar cálculo da acurácia\n",
    "    # Acurácia = (TP + TN) / (TP + TN + FP + FN)\n",
    "    # Ou simplesmente: acertos / total\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # Calcular número de acertos\n",
    "    correct = np.sum(y_true == y_pred)\n",
    "    total = len(y_true)\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297de5c8",
   "metadata": {},
   "source": [
    "## Exercício 2: Precisão (Precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643616b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_score(y_true, y_pred, pos_label=1):\n",
    "    \"\"\"\n",
    "    Calcula a precisão para classificação binária.\n",
    "\n",
    "    Parâmetros:\n",
    "    y_true (array-like): Valores reais\n",
    "    y_pred (array-like): Valores preditos\n",
    "    pos_label: Rótulo da classe positiva\n",
    "\n",
    "    Retorna:\n",
    "    float: Precisão calculada\n",
    "    \"\"\"\n",
    "    # TODO: Implementar cálculo da precisão\n",
    "    # Precisão = TP / (TP + FP)\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # Calcular True Positives e False Positives\n",
    "    tp = np.sum((y_true == pos_label) & (y_pred == pos_label))\n",
    "    fp = np.sum((y_true != pos_label) & (y_pred == pos_label))\n",
    "\n",
    "    # Evitar divisão por zero\n",
    "    if tp + fp == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return tp / (tp + fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dc9a6a",
   "metadata": {},
   "source": [
    "## Exercício 3: Recall (Sensibilidade)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efac7f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_score(y_true, y_pred, pos_label=1):\n",
    "    \"\"\"\n",
    "    Calcula o recall para classificação binária.\n",
    "\n",
    "    Parâmetros:\n",
    "    y_true (array-like): Valores reais\n",
    "    y_pred (array-like): Valores preditos\n",
    "    pos_label: Rótulo da classe positiva\n",
    "\n",
    "    Retorna:\n",
    "    float: Recall calculado\n",
    "    \"\"\"\n",
    "    # TODO: Implementar cálculo do recall\n",
    "    # Recall = TP / (TP + FN)\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # Calcular True Positives e False Negatives\n",
    "    tp = np.sum((y_true == pos_label) & (y_pred == pos_label))\n",
    "    fn = np.sum((y_true == pos_label) & (y_pred != pos_label))\n",
    "\n",
    "    # Evitar divisão por zero\n",
    "    if tp + fn == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4db77e",
   "metadata": {},
   "source": [
    "## Exercício 4: F1-Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0172a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred, pos_label=1):\n",
    "    \"\"\"\n",
    "    Calcula o F1-Score para classificação binária.\n",
    "\n",
    "    Parâmetros:\n",
    "    y_true (array-like): Valores reais\n",
    "    y_pred (array-like): Valores preditos\n",
    "    pos_label: Rótulo da classe positiva\n",
    "\n",
    "    Retorna:\n",
    "    float: F1-Score calculado\n",
    "    \"\"\"\n",
    "    # TODO: Implementar cálculo do F1-Score\n",
    "    # F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    # Dica: Use as funções precision_score e recall_score implementadas acima\n",
    "\n",
    "    precision = precision_score(y_true, y_pred, pos_label)\n",
    "    recall = recall_score(y_true, y_pred, pos_label)\n",
    "\n",
    "    # Evitar divisão por zero\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return 2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38ff5d8",
   "metadata": {},
   "source": [
    "## Exercício 5: Matriz de Confusão\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69efe48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calcula a matriz de confusão para classificação binária.\n",
    "\n",
    "    Parâmetros:\n",
    "    y_true (array-like): Valores reais\n",
    "    y_pred (array-like): Valores preditos\n",
    "\n",
    "    Retorna:\n",
    "    numpy.ndarray: Matriz de confusão 2x2\n",
    "    [[TN, FP],\n",
    "     [FN, TP]]\n",
    "    \"\"\"\n",
    "    # TODO: Implementar cálculo da matriz de confusão\n",
    "    # Assumindo classes 0 e 1\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # Calcular componentes da matriz\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))  # True Negatives\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))  # False Positives\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))  # False Negatives\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))  # True Positives\n",
    "\n",
    "    # Retornar matriz 2x2\n",
    "    return np.array([[tn, fp], [fn, tp]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd96ee7",
   "metadata": {},
   "source": [
    "## Exercício 6: Função de Avaliação Completa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053ed58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_report(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calcula todas as métricas de classificação.\n",
    "\n",
    "    Parâmetros:\n",
    "    y_true (array-like): Valores reais\n",
    "    y_pred (array-like): Valores preditos\n",
    "\n",
    "    Retorna:\n",
    "    dict: Dicionário com todas as métricas\n",
    "    \"\"\"\n",
    "    # TODO: Implementar função que retorna todas as métricas\n",
    "    # Retornar dicionário com chaves: 'accuracy', 'precision', 'recall', 'f1'\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred),\n",
    "        \"recall\": recall_score(y_true, y_pred),\n",
    "        \"f1\": f1_score(y_true, y_pred),\n",
    "        \"confusion_matrix\": confusion_matrix(y_true, y_pred),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d2ced6",
   "metadata": {},
   "source": [
    "## Exercício 7: Métricas Multiclasse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cfb538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_multiclass(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calcula acurácia para classificação multiclasse.\n",
    "\n",
    "    Parâmetros:\n",
    "    y_true (array-like): Valores reais\n",
    "    y_pred (array-like): Valores preditos\n",
    "\n",
    "    Retorna:\n",
    "    float: Acurácia multiclasse\n",
    "    \"\"\"\n",
    "    # TODO: Implementar acurácia multiclasse\n",
    "    # A fórmula é a mesma da binária: acertos / total\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    return np.sum(y_true == y_pred) / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4150cd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_per_class(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calcula precisão e recall para cada classe em classificação multiclasse.\n",
    "\n",
    "    Parâmetros:\n",
    "    y_true (array-like): Valores reais\n",
    "    y_pred (array-like): Valores preditos\n",
    "\n",
    "    Retorna:\n",
    "    dict: Dicionário com precisão e recall por classe\n",
    "    \"\"\"\n",
    "    # TODO: Implementar métricas por classe\n",
    "    # Para cada classe, calcular precisão e recall tratando como binária (one-vs-rest)\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    classes = np.unique(y_true)\n",
    "    results = {}\n",
    "\n",
    "    for cls in classes:\n",
    "        # Tratar como problema binário (classe atual vs. todas as outras)\n",
    "        y_true_binary = (y_true == cls).astype(int)\n",
    "        y_pred_binary = (y_pred == cls).astype(int)\n",
    "\n",
    "        precision = precision_score(y_true_binary, y_pred_binary)\n",
    "        recall = recall_score(y_true_binary, y_pred_binary)\n",
    "\n",
    "        results[f\"class_{cls}\"] = {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1_score(y_true_binary, y_pred_binary),\n",
    "        }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed497c6",
   "metadata": {},
   "source": [
    "## Testes das Implementações\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc827b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste com dados simples\n",
    "y_true = [1, 0, 1, 1, 0, 1, 0, 0]\n",
    "y_pred = [1, 0, 1, 0, 0, 1, 0, 1]\n",
    "\n",
    "print(\"Teste com classificação binária:\")\n",
    "print(f\"Acurácia: {accuracy_score(y_true, y_pred):.3f}\")\n",
    "print(f\"Precisão: {precision_score(y_true, y_pred):.3f}\")\n",
    "print(f\"Recall: {recall_score(y_true, y_pred):.3f}\")\n",
    "print(f\"F1-Score: {f1_score(y_true, y_pred):.3f}\")\n",
    "\n",
    "print(\"\\nMatriz de Confusão:\")\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a07d59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste com classificação perfeita\n",
    "y_true_perfect = [1, 0, 1, 1, 0]\n",
    "y_pred_perfect = [1, 0, 1, 1, 0]\n",
    "\n",
    "print(\"Teste com classificação perfeita:\")\n",
    "report = classification_report(y_true_perfect, y_pred_perfect)\n",
    "for metric, value in report.items():\n",
    "    if metric != \"confusion_matrix\":\n",
    "        print(f\"{metric}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6329680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste multiclasse\n",
    "y_true_multi = [0, 1, 2, 0, 1, 2, 0, 1, 2]\n",
    "y_pred_multi = [0, 1, 2, 0, 1, 1, 0, 2, 2]\n",
    "\n",
    "print(\"Teste com classificação multiclasse:\")\n",
    "print(f\"Acurácia multiclasse: {accuracy_multiclass(y_true_multi, y_pred_multi):.3f}\")\n",
    "\n",
    "print(\"\\nMétricas por classe:\")\n",
    "per_class = precision_recall_per_class(y_true_multi, y_pred_multi)\n",
    "for class_name, metrics in per_class.items():\n",
    "    print(\n",
    "        f\"{class_name}: Precisão={metrics['precision']:.3f}, Recall={metrics['recall']:.3f}, F1={metrics['f1']:.3f}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
