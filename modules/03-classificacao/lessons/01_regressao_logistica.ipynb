{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6492f537",
   "metadata": {},
   "source": [
    "# Regressão Logística\n",
    "\n",
    "## Objetivos\n",
    "\n",
    "- Compreender os fundamentos da regressão logística\n",
    "- Implementar regressão logística do zero\n",
    "- Usar scikit-learn para classificação\n",
    "- Interpretar resultados e probabilidades\n",
    "\n",
    "## Pré-requisitos\n",
    "\n",
    "- Fundamentos de ML\n",
    "- Regressão Linear\n",
    "- Conceitos básicos de probabilidade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8c0386",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use(\"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dd90b1",
   "metadata": {},
   "source": [
    "## 1. Conceitos Fundamentais\n",
    "\n",
    "A regressão logística é um algoritmo de classificação que usa a função logística (sigmoide) para modelar a probabilidade de uma classe.\n",
    "\n",
    "### Função Sigmoide\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "Onde $z = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cded9d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar a função sigmoide\n",
    "z = np.linspace(-10, 10, 100)\n",
    "sigmoid = 1 / (1 + np.exp(-z))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(z, sigmoid, \"b-\", linewidth=2, label=\"Função Sigmoide\")\n",
    "plt.axhline(y=0.5, color=\"r\", linestyle=\"--\", alpha=0.7, label=\"Threshold = 0.5\")\n",
    "plt.xlabel(\"z (entrada linear)\")\n",
    "plt.ylabel(\"σ(z) (probabilidade)\")\n",
    "plt.title(\"Função Sigmoide\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Propriedades da função sigmoide:\")\n",
    "print(f\"σ(-∞) ≈ {sigmoid[0]:.3f}\")\n",
    "print(f\"σ(0) = {sigmoid[50]:.3f}\")\n",
    "print(f\"σ(+∞) ≈ {sigmoid[-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4112ffc",
   "metadata": {},
   "source": [
    "## 2. Dados de Exemplo\n",
    "\n",
    "Vamos gerar um dataset sintético para classificação binária.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa32ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar dados sintéticos\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=2,\n",
    "    n_redundant=0,\n",
    "    n_informative=2,\n",
    "    n_clusters_per_class=1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(f\"Formato dos dados: X={X.shape}, y={y.shape}\")\n",
    "print(f\"Classes: {np.unique(y)}\")\n",
    "print(f\"Distribuição das classes: {np.bincount(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623a2f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar os dados\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap=\"RdYlBu\", alpha=0.7)\n",
    "plt.colorbar(scatter)\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"Dataset de Classificação Binária\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e405c8",
   "metadata": {},
   "source": [
    "## 3. Implementação do Zero\n",
    "\n",
    "Vamos implementar a regressão logística sem usar bibliotecas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fe2c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionCustom:\n",
    "    def __init__(self, learning_rate=0.01, max_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.costs = []\n",
    "\n",
    "    def _add_bias(self, X):\n",
    "        \"\"\"Adiciona coluna de bias (intercepto)\"\"\"\n",
    "        return np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"Função sigmoide com estabilidade numérica\"\"\"\n",
    "        z = np.clip(z, -250, 250)  # Evitar overflow\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def _cost_function(self, h, y):\n",
    "        \"\"\"Função de custo (log-likelihood negativa)\"\"\"\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Treinar o modelo\"\"\"\n",
    "        # Inicializar pesos\n",
    "        n_features = X.shape[1]\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        # Gradiente descendente\n",
    "        for i in range(self.max_iterations):\n",
    "            # Forward pass\n",
    "            z = X.dot(self.weights) + self.bias\n",
    "            h = self._sigmoid(z)\n",
    "\n",
    "            # Calcular custo\n",
    "            cost = self._cost_function(h, y)\n",
    "            self.costs.append(cost)\n",
    "\n",
    "            # Calcular gradientes\n",
    "            dw = X.T.dot(h - y) / y.size\n",
    "            db = (h - y).mean()\n",
    "\n",
    "            # Atualizar pesos\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predizer probabilidades\"\"\"\n",
    "        z = X.dot(self.weights) + self.bias\n",
    "        return self._sigmoid(z)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predizer classes\"\"\"\n",
    "        return (self.predict_proba(X) >= 0.5).astype(int)\n",
    "\n",
    "\n",
    "# Teste da implementação\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Treinar modelo customizado\n",
    "model_custom = LogisticRegressionCustom(learning_rate=0.1, max_iterations=1000)\n",
    "model_custom.fit(X_train, y_train)\n",
    "\n",
    "# Fazer previsões\n",
    "y_pred_custom = model_custom.predict(X_test)\n",
    "accuracy_custom = accuracy_score(y_test, y_pred_custom)\n",
    "\n",
    "print(f\"Acurácia do modelo customizado: {accuracy_custom:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329708c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar a convergência\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(model_custom.costs, \"b-\", linewidth=2)\n",
    "plt.xlabel(\"Iterações\")\n",
    "plt.ylabel(\"Custo (Log-Loss)\")\n",
    "plt.title(\"Convergência do Gradiente Descendente\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Custo inicial: {model_custom.costs[0]:.3f}\")\n",
    "print(f\"Custo final: {model_custom.costs[-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63ead0e",
   "metadata": {},
   "source": [
    "## 4. Usando Scikit-Learn\n",
    "\n",
    "Agora vamos comparar com a implementação do scikit-learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9888dabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar modelo do scikit-learn\n",
    "model_sklearn = LogisticRegression(random_state=42)\n",
    "model_sklearn.fit(X_train, y_train)\n",
    "\n",
    "# Fazer previsões\n",
    "y_pred_sklearn = model_sklearn.predict(X_test)\n",
    "y_proba_sklearn = model_sklearn.predict_proba(X_test)\n",
    "\n",
    "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
    "\n",
    "print(f\"Acurácia do scikit-learn: {accuracy_sklearn:.3f}\")\n",
    "print(f\"Acurácia do modelo customizado: {accuracy_custom:.3f}\")\n",
    "\n",
    "# Comparar pesos\n",
    "print(f\"\\nPesos (customizado): {model_custom.weights}\")\n",
    "print(f\"Pesos (sklearn): {model_sklearn.coef_[0]}\")\n",
    "print(f\"Bias (customizado): {model_custom.bias:.3f}\")\n",
    "print(f\"Bias (sklearn): {model_sklearn.intercept_[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6e3718",
   "metadata": {},
   "source": [
    "## 5. Visualização da Fronteira de Decisão\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb097f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(X, y, model, title=\"Fronteira de Decisão\"):\n",
    "    \"\"\"Plotar fronteira de decisão\"\"\"\n",
    "    h = 0.01\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        if hasattr(model, \"classes_\"):  # sklearn\n",
    "            Z = model.predict_proba(mesh_points)[:, 1]\n",
    "        else:  # modelo customizado\n",
    "            Z = model.predict_proba(mesh_points)\n",
    "    else:\n",
    "        Z = model.predict(mesh_points)\n",
    "\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Subplot 1: Fronteira de decisão\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.contourf(xx, yy, Z, levels=50, alpha=0.8, cmap=\"RdYlBu\")\n",
    "    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap=\"RdYlBu\", edgecolors=\"black\")\n",
    "    plt.colorbar(scatter)\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.title(f\"{title} - Probabilidades\")\n",
    "\n",
    "    # Subplot 2: Linha de decisão\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.contour(xx, yy, Z, levels=[0.5], colors=\"black\", linestyles=\"--\", linewidths=2)\n",
    "    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap=\"RdYlBu\", edgecolors=\"black\")\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.title(f\"{title} - Linha de Decisão\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Visualizar ambos os modelos\n",
    "plot_decision_boundary(X_test, y_test, model_sklearn, \"Scikit-Learn\")\n",
    "plot_decision_boundary(X_test, y_test, model_custom, \"Implementação Customizada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5c53ad",
   "metadata": {},
   "source": [
    "## 6. Métricas de Avaliação\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd90fab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusão\n",
    "def plot_confusion_matrix(y_true, y_pred, title=\"Matriz de Confusão\"):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=[\"Classe 0\", \"Classe 1\"],\n",
    "        yticklabels=[\"Classe 0\", \"Classe 1\"],\n",
    "    )\n",
    "    plt.xlabel(\"Predito\")\n",
    "    plt.ylabel(\"Real\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "    return cm\n",
    "\n",
    "\n",
    "cm = plot_confusion_matrix(y_test, y_pred_sklearn)\n",
    "\n",
    "# Relatório de classificação\n",
    "print(\"Relatório de Classificação:\")\n",
    "print(classification_report(y_test, y_pred_sklearn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71693cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise das probabilidades\n",
    "probabilities = model_sklearn.predict_proba(X_test)[:, 1]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Subplot 1: Distribuição das probabilidades\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(probabilities[y_test == 0], bins=30, alpha=0.7, label=\"Classe 0\", color=\"blue\")\n",
    "plt.hist(probabilities[y_test == 1], bins=30, alpha=0.7, label=\"Classe 1\", color=\"red\")\n",
    "plt.axvline(x=0.5, color=\"black\", linestyle=\"--\", label=\"Threshold\")\n",
    "plt.xlabel(\"Probabilidade Predita\")\n",
    "plt.ylabel(\"Frequência\")\n",
    "plt.title(\"Distribuição das Probabilidades\")\n",
    "plt.legend()\n",
    "\n",
    "# Subplot 2: Curva ROC (simplificada)\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, probabilities)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(fpr, tpr, color=\"darkorange\", lw=2, label=f\"ROC (AUC = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\", label=\"Random\")\n",
    "plt.xlabel(\"Taxa de Falsos Positivos\")\n",
    "plt.ylabel(\"Taxa de Verdadeiros Positivos\")\n",
    "plt.title(\"Curva ROC\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"AUC-ROC: {roc_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d566d45",
   "metadata": {},
   "source": [
    "## 7. Regressão Logística Multiclasse\n",
    "\n",
    "Exemplo com mais de 2 classes usando One-vs-Rest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759d8489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar dados multiclasse\n",
    "X_multi, y_multi = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=2,\n",
    "    n_redundant=0,\n",
    "    n_informative=2,\n",
    "    n_classes=3,\n",
    "    n_clusters_per_class=1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(\n",
    "    X_multi, y_multi, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Treinar modelo multiclasse\n",
    "model_multi = LogisticRegression(random_state=42, multi_class=\"ovr\")\n",
    "model_multi.fit(X_train_multi, y_train_multi)\n",
    "\n",
    "y_pred_multi = model_multi.predict(X_test_multi)\n",
    "accuracy_multi = accuracy_score(y_test_multi, y_pred_multi)\n",
    "\n",
    "print(f\"Acurácia multiclasse: {accuracy_multi:.3f}\")\n",
    "print(f\"Classes: {model_multi.classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbf9e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar classificação multiclasse\n",
    "plot_decision_boundary(\n",
    "    X_test_multi, y_test_multi, model_multi, \"Classificação Multiclasse\"\n",
    ")\n",
    "\n",
    "# Matriz de confusão multiclasse\n",
    "cm_multi = plot_confusion_matrix(\n",
    "    y_test_multi, y_pred_multi, \"Matriz de Confusão - Multiclasse\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a29783",
   "metadata": {},
   "source": [
    "## 8. Resumo e Conclusões\n",
    "\n",
    "### Pontos-chave da Regressão Logística:\n",
    "\n",
    "1. **Função Sigmoide**: Mapeia qualquer valor real para [0,1]\n",
    "2. **Interpretabilidade**: Coeficientes representam log-odds\n",
    "3. **Probabilidades**: Fornece probabilidades calibradas\n",
    "4. **Flexibilidade**: Funciona para classificação binária e multiclasse\n",
    "5. **Eficiência**: Rápido para treinar e predizer\n",
    "\n",
    "### Vantagens:\n",
    "\n",
    "- Simples e interpretável\n",
    "- Não requer tuning de hiperparâmetros\n",
    "- Menos propenso a overfitting\n",
    "- Fornece probabilidades\n",
    "\n",
    "### Limitações:\n",
    "\n",
    "- Assume relação linear entre features e log-odds\n",
    "- Sensível a outliers\n",
    "- Requer features com escalas similares\n",
    "- Não captura interações complexas\n",
    "\n",
    "### Próximos Passos:\n",
    "\n",
    "- Support Vector Machines\n",
    "- Árvores de Decisão\n",
    "- Ensemble Methods\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
