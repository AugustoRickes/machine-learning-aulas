{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adbca636",
   "metadata": {},
   "source": [
    "# Validação Cruzada em Machine Learning\n",
    "\n",
    "## Objetivos\n",
    "\n",
    "- Compreender a importância da validação cruzada\n",
    "- Implementar diferentes tipos de validação cruzada\n",
    "- Avaliar a estabilidade de modelos\n",
    "- Evitar overfitting através de validação adequada\n",
    "\n",
    "## Pré-requisitos\n",
    "\n",
    "- Conceitos básicos de ML\n",
    "- Divisão treino/teste\n",
    "- Métricas de avaliação\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a29a026",
   "metadata": {},
   "source": [
    "## 1. Por que Validação Cruzada?\n",
    "\n",
    "A validação cruzada é uma técnica fundamental para:\n",
    "\n",
    "- **Estimar performance real** do modelo\n",
    "- **Detectar overfitting** de forma mais robusta\n",
    "- **Comparar modelos** de forma justa\n",
    "- **Usar dados limitados** de forma eficiente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f816791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold, TimeSeriesSplit, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279df72f",
   "metadata": {},
   "source": [
    "## 2. Problema da Divisão Simples Treino/Teste\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908910cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar dataset de exemplo\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=10, random_state=42)\n",
    "\n",
    "# Simular múltiplas divisões treino/teste\n",
    "scores = []\n",
    "for i in range(100):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=i)\n",
    "\n",
    "    model = LogisticRegression(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    score = model.score(X_test, y_test)\n",
    "    scores.append(score)\n",
    "\n",
    "print(f\"Accuracy média: {np.mean(scores):.3f}\")\n",
    "print(f\"Desvio padrão: {np.std(scores):.3f}\")\n",
    "print(f\"Variação: {np.min(scores):.3f} - {np.max(scores):.3f}\")\n",
    "\n",
    "# Visualizar distribuição\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(scores, bins=20, alpha=0.7, edgecolor=\"black\")\n",
    "plt.axvline(np.mean(scores), color=\"red\", linestyle=\"--\", label=f\"Média: {np.mean(scores):.3f}\")\n",
    "plt.xlabel(\"Accuracy\")\n",
    "plt.ylabel(\"Frequência\")\n",
    "plt.title(\"Variabilidade da Performance com Diferentes Divisões Treino/Teste\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1090f65",
   "metadata": {},
   "source": [
    "## 3. K-Fold Cross Validation\n",
    "\n",
    "O K-Fold divide os dados em K partes (folds) e usa cada parte como teste uma vez.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267ccd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementação manual do K-Fold\n",
    "def manual_kfold_cv(X, y, model, k=5):\n",
    "    n_samples = len(X)\n",
    "    fold_size = n_samples // k\n",
    "    scores = []\n",
    "\n",
    "    for i in range(k):\n",
    "        # Definir índices do fold de teste\n",
    "        start_idx = i * fold_size\n",
    "        if i == k - 1:  # Último fold pega o resto\n",
    "            end_idx = n_samples\n",
    "        else:\n",
    "            end_idx = (i + 1) * fold_size\n",
    "\n",
    "        # Dividir dados\n",
    "        test_indices = range(start_idx, end_idx)\n",
    "        train_indices = list(range(0, start_idx)) + list(range(end_idx, n_samples))\n",
    "\n",
    "        X_train_fold = X[train_indices]\n",
    "        X_test_fold = X[test_indices]\n",
    "        y_train_fold = y[train_indices]\n",
    "        y_test_fold = y[test_indices]\n",
    "\n",
    "        # Treinar e avaliar\n",
    "        model_copy = LogisticRegression(random_state=42)\n",
    "        model_copy.fit(X_train_fold, y_train_fold)\n",
    "        score = model_copy.score(X_test_fold, y_test_fold)\n",
    "        scores.append(score)\n",
    "\n",
    "        print(f\"Fold {i+1}: Accuracy = {score:.3f}\")\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "# Testar implementação manual\n",
    "print(\"=== Implementação Manual K-Fold ===\")\n",
    "manual_scores = manual_kfold_cv(X, y, LogisticRegression(), k=5)\n",
    "print(f\"\\nMédia: {np.mean(manual_scores):.3f} ± {np.std(manual_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db9eb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar sklearn para K-Fold\n",
    "print(\"\\n=== Sklearn K-Fold ===\")\n",
    "model = LogisticRegression(random_state=42)\n",
    "cv_scores = cross_val_score(model, X, y, cv=5, scoring=\"accuracy\")\n",
    "\n",
    "for i, score in enumerate(cv_scores):\n",
    "    print(f\"Fold {i+1}: Accuracy = {score:.3f}\")\n",
    "\n",
    "print(f\"\\nMédia: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23fead5",
   "metadata": {},
   "source": [
    "## 4. Stratified K-Fold\n",
    "\n",
    "Mantém a proporção das classes em cada fold - essencial para datasets desbalanceados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3c0447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar dataset desbalanceado\n",
    "X_imb, y_imb = make_classification(n_samples=1000, n_features=20, weights=[0.9, 0.1], random_state=42)\n",
    "\n",
    "print(\"Distribuição das classes:\")\n",
    "unique, counts = np.unique(y_imb, return_counts=True)\n",
    "for cls, count in zip(unique, counts):\n",
    "    print(f\"Classe {cls}: {count} ({count/len(y_imb)*100:.1f}%)\")\n",
    "\n",
    "# Comparar K-Fold regular vs Stratified\n",
    "print(\"\\n=== K-Fold Regular ===\")\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for i, (train_idx, test_idx) in enumerate(kfold.split(X_imb)):\n",
    "    y_train_fold = y_imb[train_idx]\n",
    "    y_test_fold = y_imb[test_idx]\n",
    "\n",
    "    train_dist = np.bincount(y_train_fold) / len(y_train_fold)\n",
    "    test_dist = np.bincount(y_test_fold) / len(y_test_fold)\n",
    "\n",
    "    print(\n",
    "        f\"Fold {i+1} - Treino: [{train_dist[0]:.2f}, {train_dist[1]:.2f}] \"\n",
    "        f\"Teste: [{test_dist[0]:.2f}, {test_dist[1]:.2f}]\"\n",
    "    )\n",
    "\n",
    "print(\"\\n=== Stratified K-Fold ===\")\n",
    "skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for i, (train_idx, test_idx) in enumerate(skfold.split(X_imb, y_imb)):\n",
    "    y_train_fold = y_imb[train_idx]\n",
    "    y_test_fold = y_imb[test_idx]\n",
    "\n",
    "    train_dist = np.bincount(y_train_fold) / len(y_train_fold)\n",
    "    test_dist = np.bincount(y_test_fold) / len(y_test_fold)\n",
    "\n",
    "    print(\n",
    "        f\"Fold {i+1} - Treino: [{train_dist[0]:.2f}, {train_dist[1]:.2f}] \"\n",
    "        f\"Teste: [{test_dist[0]:.2f}, {test_dist[1]:.2f}]\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953cd176",
   "metadata": {},
   "source": [
    "## 5. Time Series Split\n",
    "\n",
    "Para dados temporais, não podemos misturar passado e futuro!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7514fe75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simular dados de série temporal\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "time = np.arange(n_samples)\n",
    "trend = 0.01 * time\n",
    "seasonal = 2 * np.sin(2 * np.pi * time / 50)\n",
    "noise = np.random.normal(0, 0.5, n_samples)\n",
    "y_ts = trend + seasonal + noise\n",
    "\n",
    "\n",
    "# Criar features baseadas em lag\n",
    "def create_lag_features(y, n_lags=5):\n",
    "    X = np.zeros((len(y) - n_lags, n_lags))\n",
    "    for i in range(n_lags):\n",
    "        X[:, i] = y[i : len(y) - n_lags + i]\n",
    "    return X\n",
    "\n",
    "\n",
    "X_ts = create_lag_features(y_ts, n_lags=5)\n",
    "y_ts_target = y_ts[5:]  # Target é o próximo valor\n",
    "\n",
    "print(f\"Shape dos dados temporais: X={X_ts.shape}, y={y_ts_target.shape}\")\n",
    "\n",
    "# Visualizar Time Series Split\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "for i, (train_idx, test_idx) in enumerate(tscv.split(X_ts)):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.plot(train_idx, y_ts_target[train_idx], \"b-\", label=\"Treino\", alpha=0.7)\n",
    "    plt.plot(test_idx, y_ts_target[test_idx], \"r-\", label=\"Teste\", alpha=0.7)\n",
    "    plt.title(f\"Split {i+1}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Avaliar com Time Series CV\n",
    "model_ts = LinearRegression()\n",
    "cv_scores_ts = cross_val_score(\n",
    "    model_ts, X_ts, y_ts_target, cv=TimeSeriesSplit(n_splits=5), scoring=\"neg_mean_squared_error\"\n",
    ")\n",
    "\n",
    "print(f\"\\nTime Series CV - MSE: {-cv_scores_ts.mean():.3f} ± {cv_scores_ts.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f238dd1e",
   "metadata": {},
   "source": [
    "## 6. Comparando Modelos com Cross Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda4eb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar diferentes modelos\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"SVM\": SVC(random_state=42),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "}\n",
    "\n",
    "# Avaliar cada modelo com CV\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    cv_scores = cross_val_score(model, X, y, cv=5, scoring=\"accuracy\")\n",
    "    results[name] = cv_scores\n",
    "    print(f\"{name:20}: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "\n",
    "# Visualizar comparação\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.boxplot(results.values(), labels=results.keys())\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Comparação de Modelos com Cross Validation\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49690e3c",
   "metadata": {},
   "source": [
    "## 7. Nested Cross Validation\n",
    "\n",
    "Para seleção de hiperparâmetros sem vazamento de dados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea9b464",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Nested CV para Random Forest\n",
    "def nested_cross_validation(X, y, outer_cv=5, inner_cv=3):\n",
    "    # Parâmetros para busca\n",
    "    param_grid = {\"n_estimators\": [50, 100, 200], \"max_depth\": [3, 5, 7, None]}\n",
    "\n",
    "    # CV externo\n",
    "    outer_scores = []\n",
    "    kfold_outer = KFold(n_splits=outer_cv, shuffle=True, random_state=42)\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kfold_outer.split(X)):\n",
    "        X_train_outer, X_test_outer = X[train_idx], X[test_idx]\n",
    "        y_train_outer, y_test_outer = y[train_idx], y[test_idx]\n",
    "\n",
    "        # CV interno para seleção de hiperparâmetros\n",
    "        rf = RandomForestClassifier(random_state=42)\n",
    "        grid_search = GridSearchCV(rf, param_grid, cv=inner_cv, scoring=\"accuracy\", n_jobs=-1)\n",
    "\n",
    "        # Encontrar melhores parâmetros no conjunto de treino\n",
    "        grid_search.fit(X_train_outer, y_train_outer)\n",
    "\n",
    "        # Avaliar no conjunto de teste\n",
    "        best_model = grid_search.best_estimator_\n",
    "        score = best_model.score(X_test_outer, y_test_outer)\n",
    "        outer_scores.append(score)\n",
    "\n",
    "        print(f\"Fold {fold+1}: Score = {score:.3f}, \" f\"Best params = {grid_search.best_params_}\")\n",
    "\n",
    "    return outer_scores\n",
    "\n",
    "\n",
    "print(\"=== Nested Cross Validation ===\")\n",
    "nested_scores = nested_cross_validation(X, y)\n",
    "print(f\"\\nPerformance estimada: {np.mean(nested_scores):.3f} ± {np.std(nested_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0141e4d4",
   "metadata": {},
   "source": [
    "## 8. Resumo e Boas Práticas\n",
    "\n",
    "### Escolhendo o Tipo de CV:\n",
    "\n",
    "- **K-Fold**: Dados gerais, balanceados\n",
    "- **Stratified K-Fold**: Classificação com classes desbalanceadas\n",
    "- **Time Series Split**: Dados temporais\n",
    "- **Leave-One-Out**: Datasets muito pequenos\n",
    "\n",
    "### Parâmetros Importantes:\n",
    "\n",
    "- **K=5 ou K=10**: Padrão para a maioria dos casos\n",
    "- **shuffle=True**: Importante para dados ordenados\n",
    "- **random_state**: Para reprodutibilidade\n",
    "\n",
    "### Cuidados:\n",
    "\n",
    "- Nested CV para seleção de hiperparâmetros\n",
    "- Mesmo pré-processamento em todos os folds\n",
    "- Não vazar informação entre treino e teste\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
