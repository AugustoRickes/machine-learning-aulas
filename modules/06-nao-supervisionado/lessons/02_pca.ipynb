{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d3f237f",
   "metadata": {},
   "source": [
    "# PCA e Redução de Dimensionalidade\n",
    "\n",
    "## Objetivos\n",
    "\n",
    "- Compreender o problema da alta dimensionalidade\n",
    "- Implementar e aplicar PCA (Principal Component Analysis)\n",
    "- Interpretar componentes principais\n",
    "- Aplicar PCA para visualização e pré-processamento\n",
    "- Comparar com outras técnicas de redução de dimensionalidade\n",
    "\n",
    "## Pré-requisitos\n",
    "\n",
    "- Álgebra linear básica (vetores, matrizes)\n",
    "- Conceitos de variância e covariância\n",
    "- Normalização de dados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a14592",
   "metadata": {},
   "source": [
    "## 1. O Problema da Alta Dimensionalidade\n",
    "\n",
    "### Curse of Dimensionality:\n",
    "\n",
    "- **Esparsidade**: Pontos ficam muito distantes em alta dimensão\n",
    "- **Overfitting**: Modelos se tornam complexos demais\n",
    "- **Visualização**: Impossível visualizar dados com >3 dimensões\n",
    "- **Computação**: Algoritmos ficam lentos\n",
    "\n",
    "### Soluções:\n",
    "\n",
    "- **Feature Selection**: Escolher features mais importantes\n",
    "- **Feature Extraction**: Criar novas features (PCA, t-SNE)\n",
    "- **Regularização**: Penalizar complexidade do modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1079e3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_digits, load_breast_cancer, make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.manifold import TSNE\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configurar visualização\n",
    "plt.style.use(\"default\")\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35aaf70",
   "metadata": {},
   "source": [
    "## 2. Demonstrando a Curse of Dimensionality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a64d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrar como distâncias mudam com dimensionalidade\n",
    "def analyze_distances_by_dimension(n_samples=1000, max_dim=50):\n",
    "    dimensions = range(1, max_dim + 1, 5)\n",
    "    mean_distances = []\n",
    "    std_distances = []\n",
    "\n",
    "    for dim in dimensions:\n",
    "        # Gerar pontos aleatórios\n",
    "        points = np.random.normal(0, 1, (n_samples, dim))\n",
    "\n",
    "        # Calcular distâncias do primeiro ponto para todos os outros\n",
    "        distances = np.sqrt(np.sum((points[0] - points[1:]) ** 2, axis=1))\n",
    "\n",
    "        mean_distances.append(np.mean(distances))\n",
    "        std_distances.append(np.std(distances))\n",
    "\n",
    "    return dimensions, mean_distances, std_distances\n",
    "\n",
    "\n",
    "# Analisar distâncias\n",
    "dims, means, stds = analyze_distances_by_dimension()\n",
    "\n",
    "# Plotar resultados\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(dims, means, \"b-o\", label=\"Distância Média\")\n",
    "plt.fill_between(dims, np.array(means) - np.array(stds), np.array(means) + np.array(stds), alpha=0.3)\n",
    "plt.xlabel(\"Dimensionalidade\")\n",
    "plt.ylabel(\"Distância Euclidiana\")\n",
    "plt.title(\"Distância Média entre Pontos Aleatórios\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# Razão entre distância máxima e mínima\n",
    "plt.subplot(1, 2, 2)\n",
    "relative_variation = np.array(stds) / np.array(means)\n",
    "plt.plot(dims, relative_variation, \"r-o\")\n",
    "plt.xlabel(\"Dimensionalidade\")\n",
    "plt.ylabel(\"Variação Relativa (std/mean)\")\n",
    "plt.title(\"Variabilidade Relativa das Distâncias\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observações sobre Curse of Dimensionality:\")\n",
    "print(f\"- Em alta dimensão, distâncias tendem a convergir\")\n",
    "print(f\"- Variação relativa diminui: {relative_variation[0]:.3f} (2D) -> {relative_variation[-1]:.3f} ({dims[-1]}D)\")\n",
    "print(f\"- Conceito de 'vizinhança próxima' perde significado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cbe8e1",
   "metadata": {},
   "source": [
    "## 3. Principal Component Analysis (PCA)\n",
    "\n",
    "### Ideia Central:\n",
    "\n",
    "- Encontrar **direções de máxima variância** nos dados\n",
    "- **Projetar dados** nessas direções (componentes principais)\n",
    "- **Reduzir dimensionalidade** mantendo informação importante\n",
    "\n",
    "### Algoritmo:\n",
    "\n",
    "1. **Centralizar dados** (média = 0)\n",
    "2. **Calcular matriz de covariância**\n",
    "3. **Encontrar autovalores e autovetores**\n",
    "4. **Ordenar por autovalor** (maior variância primeiro)\n",
    "5. **Projetar dados** nos autovetores escolhidos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a792142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementação manual do PCA (para compreensão)\n",
    "class PCAManual:\n",
    "    def __init__(self, n_components=2):\n",
    "        self.n_components = n_components\n",
    "\n",
    "    def fit(self, X):\n",
    "        # 1. Centralizar dados\n",
    "        self.mean_ = np.mean(X, axis=0)\n",
    "        X_centered = X - self.mean_\n",
    "\n",
    "        # 2. Calcular matriz de covariância\n",
    "        cov_matrix = np.cov(X_centered.T)\n",
    "\n",
    "        # 3. Calcular autovalores e autovetores\n",
    "        eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "        # 4. Ordenar por autovalor (maior primeiro)\n",
    "        idx = np.argsort(eigenvalues)[::-1]\n",
    "        self.eigenvalues_ = eigenvalues[idx]\n",
    "        self.eigenvectors_ = eigenvectors[:, idx]\n",
    "\n",
    "        # 5. Selecionar componentes principais\n",
    "        self.components_ = self.eigenvectors_[:, : self.n_components].T\n",
    "\n",
    "        # Calcular variância explicada\n",
    "        total_variance = np.sum(self.eigenvalues_)\n",
    "        self.explained_variance_ratio_ = self.eigenvalues_[: self.n_components] / total_variance\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Centralizar e projetar\n",
    "        X_centered = X - self.mean_\n",
    "        return X_centered @ self.components_.T\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        return self.fit(X).transform(X)\n",
    "\n",
    "    def inverse_transform(self, X_pca):\n",
    "        # Reconstruir dados originais (aproximação)\n",
    "        return X_pca @ self.components_ + self.mean_\n",
    "\n",
    "\n",
    "# Criar dados 2D simples para visualização\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "X_simple = np.random.multivariate_normal(mean=[0, 0], cov=[[3, 1.5], [1.5, 1]], size=n_samples)\n",
    "\n",
    "# Aplicar PCA manual\n",
    "pca_manual = PCAManual(n_components=2)\n",
    "X_pca_manual = pca_manual.fit_transform(X_simple)\n",
    "\n",
    "# Comparar com sklearn\n",
    "pca_sklearn = PCA(n_components=2)\n",
    "X_pca_sklearn = pca_sklearn.fit_transform(X_simple)\n",
    "\n",
    "# Visualizar\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Dados originais com componentes principais\n",
    "axes[0].scatter(X_simple[:, 0], X_simple[:, 1], alpha=0.7)\n",
    "axes[0].set_xlabel(\"Feature 1\")\n",
    "axes[0].set_ylabel(\"Feature 2\")\n",
    "axes[0].set_title(\"Dados Originais\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axis(\"equal\")\n",
    "\n",
    "# Adicionar vetores dos componentes principais\n",
    "origin = pca_manual.mean_\n",
    "for i, (component, variance) in enumerate(zip(pca_manual.components_, pca_manual.explained_variance_ratio_)):\n",
    "    axes[0].arrow(\n",
    "        origin[0],\n",
    "        origin[1],\n",
    "        component[0] * 3,\n",
    "        component[1] * 3,\n",
    "        head_width=0.1,\n",
    "        head_length=0.1,\n",
    "        fc=f\"C{i}\",\n",
    "        ec=f\"C{i}\",\n",
    "        label=f\"PC{i+1} ({variance:.1%} var)\",\n",
    "    )\n",
    "axes[0].legend()\n",
    "\n",
    "# Dados transformados (manual)\n",
    "axes[1].scatter(X_pca_manual[:, 0], X_pca_manual[:, 1], alpha=0.7)\n",
    "axes[1].set_xlabel(\"PC1\")\n",
    "axes[1].set_ylabel(\"PC2\")\n",
    "axes[1].set_title(\"PCA Manual\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axis(\"equal\")\n",
    "\n",
    "# Dados transformados (sklearn)\n",
    "axes[2].scatter(X_pca_sklearn[:, 0], X_pca_sklearn[:, 1], alpha=0.7)\n",
    "axes[2].set_xlabel(\"PC1\")\n",
    "axes[2].set_ylabel(\"PC2\")\n",
    "axes[2].set_title(\"PCA Sklearn\")\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].axis(\"equal\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Variância explicada (manual): {pca_manual.explained_variance_ratio_}\")\n",
    "print(f\"Variância explicada (sklearn): {pca_sklearn.explained_variance_ratio_}\")\n",
    "print(f\"Diferença máxima: {np.max(np.abs(np.abs(X_pca_manual) - np.abs(X_pca_sklearn))):.10f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8173d91d",
   "metadata": {},
   "source": [
    "## 4. PCA em Dataset Real: Dígitos Manuscritos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53806692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dataset de dígitos\n",
    "digits = load_digits()\n",
    "X_digits, y_digits = digits.data, digits.target\n",
    "\n",
    "print(f\"Dataset de dígitos:\")\n",
    "print(f\"Shape: {X_digits.shape}\")\n",
    "print(f\"Classes: {np.unique(y_digits)}\")\n",
    "print(f\"Cada imagem: 8x8 pixels = 64 features\")\n",
    "\n",
    "# Visualizar algumas imagens\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 6))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(digits.images[i], cmap=\"gray\")\n",
    "    ax.set_title(f\"Dígito: {y_digits[i]}\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Exemplos do Dataset de Dígitos\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Normalizar dados\n",
    "scaler = StandardScaler()\n",
    "X_digits_scaled = scaler.fit_transform(X_digits)\n",
    "\n",
    "# Aplicar PCA com diferentes números de componentes\n",
    "n_components_list = [2, 5, 10, 20, 30, 40, 50, 60]\n",
    "explained_variances = []\n",
    "\n",
    "for n_comp in n_components_list:\n",
    "    pca = PCA(n_components=n_comp)\n",
    "    pca.fit(X_digits_scaled)\n",
    "    explained_variances.append(np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "# Plotar variância explicada acumulada\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_components_list, explained_variances, \"bo-\", linewidth=2, markersize=8)\n",
    "plt.xlabel(\"Número de Componentes\")\n",
    "plt.ylabel(\"Variância Explicada Acumulada\")\n",
    "plt.title(\"Variância Explicada vs Número de Componentes\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Adicionar linhas de referência\n",
    "plt.axhline(y=0.8, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"80% variância\")\n",
    "plt.axhline(y=0.9, color=\"orange\", linestyle=\"--\", alpha=0.7, label=\"90% variância\")\n",
    "plt.axhline(y=0.95, color=\"green\", linestyle=\"--\", alpha=0.7, label=\"95% variância\")\n",
    "plt.legend()\n",
    "\n",
    "# Anotar pontos importantes\n",
    "for i, (n_comp, var_exp) in enumerate(zip(n_components_list, explained_variances)):\n",
    "    if var_exp >= 0.8 and explained_variances[i - 1] < 0.8:\n",
    "        plt.annotate(\n",
    "            f\"{n_comp} comp.\\n{var_exp:.1%}\",\n",
    "            xy=(n_comp, var_exp),\n",
    "            xytext=(n_comp + 10, var_exp - 0.05),\n",
    "            arrowprops=dict(arrowstyle=\"->\", color=\"red\"),\n",
    "        )\n",
    "        break\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVariância explicada por número de componentes:\")\n",
    "for n_comp, var_exp in zip(n_components_list, explained_variances):\n",
    "    print(f\"{n_comp:2d} componentes: {var_exp:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e807e65",
   "metadata": {},
   "source": [
    "## 5. Visualização com PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95694935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar PCA para visualização (2D e 3D)\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_digits_2d = pca_2d.fit_transform(X_digits_scaled)\n",
    "\n",
    "pca_3d = PCA(n_components=3)\n",
    "X_digits_3d = pca_3d.fit_transform(X_digits_scaled)\n",
    "\n",
    "# Visualização 2D\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# PCA 2D - colorido por classe\n",
    "plt.subplot(1, 3, 1)\n",
    "scatter = plt.scatter(X_digits_2d[:, 0], X_digits_2d[:, 1], c=y_digits, cmap=\"tab10\", alpha=0.7, s=20)\n",
    "plt.xlabel(f\"PC1 ({pca_2d.explained_variance_ratio_[0]:.1%} variância)\")\n",
    "plt.ylabel(f\"PC2 ({pca_2d.explained_variance_ratio_[1]:.1%} variância)\")\n",
    "plt.title(\"PCA 2D - Dígitos\")\n",
    "plt.colorbar(scatter, label=\"Dígito\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Destacar alguns dígitos específicos\n",
    "plt.subplot(1, 3, 2)\n",
    "for digit in [0, 1, 4, 7]:\n",
    "    mask = y_digits == digit\n",
    "    plt.scatter(X_digits_2d[mask, 0], X_digits_2d[mask, 1], label=f\"Dígito {digit}\", alpha=0.7, s=20)\n",
    "plt.xlabel(f\"PC1\")\n",
    "plt.ylabel(f\"PC2\")\n",
    "plt.title(\"PCA 2D - Dígitos Selecionados\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Densidade por região\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hexbin(X_digits_2d[:, 0], X_digits_2d[:, 1], gridsize=30, cmap=\"Blues\")\n",
    "plt.xlabel(f\"PC1\")\n",
    "plt.ylabel(f\"PC2\")\n",
    "plt.title(\"Densidade de Pontos\")\n",
    "plt.colorbar(label=\"Contagem\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"PCA 2D - Variância total explicada: {np.sum(pca_2d.explained_variance_ratio_):.1%}\")\n",
    "print(f\"PCA 3D - Variância total explicada: {np.sum(pca_3d.explained_variance_ratio_):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab282a86",
   "metadata": {},
   "source": [
    "## 6. Reconstrução de Imagens com PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ab74cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruir imagens com diferentes números de componentes\n",
    "def reconstruct_images(X, n_components_list, image_idx=0):\n",
    "    \"\"\"Reconstrói uma imagem usando diferentes números de componentes PCA\"\"\"\n",
    "    original_image = X[image_idx].reshape(8, 8)\n",
    "    reconstructions = []\n",
    "\n",
    "    for n_comp in n_components_list:\n",
    "        pca = PCA(n_components=n_comp)\n",
    "        X_pca = pca.fit_transform(X)\n",
    "        X_reconstructed = pca.inverse_transform(X_pca)\n",
    "\n",
    "        reconstructed_image = X_reconstructed[image_idx].reshape(8, 8)\n",
    "        reconstructions.append(reconstructed_image)\n",
    "\n",
    "    return original_image, reconstructions\n",
    "\n",
    "\n",
    "# Testar reconstrução\n",
    "test_image_idx = 0\n",
    "n_components_test = [1, 2, 5, 10, 20, 40]\n",
    "\n",
    "original, reconstructed_list = reconstruct_images(X_digits_scaled, n_components_test, test_image_idx)\n",
    "\n",
    "# Visualizar reconstruções\n",
    "fig, axes = plt.subplots(2, len(n_components_test) + 1, figsize=(18, 6))\n",
    "\n",
    "# Linha superior: reconstruções\n",
    "axes[0, 0].imshow(digits.images[test_image_idx], cmap=\"gray\")\n",
    "axes[0, 0].set_title(f\"Original\\nDígito {y_digits[test_image_idx]}\")\n",
    "axes[0, 0].axis(\"off\")\n",
    "\n",
    "for i, (n_comp, reconstructed) in enumerate(zip(n_components_test, reconstructed_list)):\n",
    "    axes[0, i + 1].imshow(reconstructed, cmap=\"gray\")\n",
    "    axes[0, i + 1].set_title(f\"{n_comp} comp.\")\n",
    "    axes[0, i + 1].axis(\"off\")\n",
    "\n",
    "# Linha inferior: erro de reconstrução\n",
    "axes[1, 0].text(0.5, 0.5, \"Erro de\\nReconstrução\", ha=\"center\", va=\"center\", transform=axes[1, 0].transAxes)\n",
    "axes[1, 0].axis(\"off\")\n",
    "\n",
    "for i, (n_comp, reconstructed) in enumerate(zip(n_components_test, reconstructed_list)):\n",
    "    error = np.abs(digits.images[test_image_idx] - reconstructed)\n",
    "    im = axes[1, i + 1].imshow(error, cmap=\"Reds\")\n",
    "    axes[1, i + 1].set_title(f\"MSE: {np.mean(error**2):.3f}\")\n",
    "    axes[1, i + 1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calcular MSE para diferentes números de componentes\n",
    "mse_scores = []\n",
    "for n_comp in range(1, 65):\n",
    "    pca = PCA(n_components=n_comp)\n",
    "    X_pca = pca.fit_transform(X_digits_scaled)\n",
    "    X_reconstructed = pca.inverse_transform(X_pca)\n",
    "    mse = np.mean((X_digits_scaled - X_reconstructed) ** 2)\n",
    "    mse_scores.append(mse)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, 65), mse_scores, \"b-\", linewidth=2)\n",
    "plt.xlabel(\"Número de Componentes\")\n",
    "plt.ylabel(\"MSE de Reconstrução\")\n",
    "plt.title(\"Erro de Reconstrução vs Número de Componentes\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTrade-off entre compressão e qualidade:\")\n",
    "for n_comp in [5, 10, 20, 30]:\n",
    "    compression_ratio = n_comp / 64\n",
    "    mse = mse_scores[n_comp - 1]\n",
    "    print(f\"{n_comp:2d} componentes: {compression_ratio:.1%} dos dados originais, MSE = {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c880a0",
   "metadata": {},
   "source": [
    "## 7. PCA como Pré-processamento para ML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be98955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar performance de ML com e sem PCA\n",
    "def compare_ml_with_pca(X, y, n_components_list, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Compara performance de modelos ML com diferentes números de componentes PCA\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42, stratify=y)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Modelo sem PCA\n",
    "    rf_baseline = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_baseline.fit(X_train, y_train)\n",
    "    baseline_score = rf_baseline.score(X_test, y_test)\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            \"n_components\": \"Original (64)\",\n",
    "            \"accuracy\": baseline_score,\n",
    "            \"n_features\": X.shape[1],\n",
    "            \"compression_ratio\": 1.0,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Modelos com PCA\n",
    "    for n_comp in n_components_list:\n",
    "        # Aplicar PCA\n",
    "        pca = PCA(n_components=n_comp)\n",
    "        X_train_pca = pca.fit_transform(X_train)\n",
    "        X_test_pca = pca.transform(X_test)\n",
    "\n",
    "        # Treinar modelo\n",
    "        rf_pca = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        rf_pca.fit(X_train_pca, y_train)\n",
    "        pca_score = rf_pca.score(X_test_pca, y_test)\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"n_components\": n_comp,\n",
    "                \"accuracy\": pca_score,\n",
    "                \"n_features\": n_comp,\n",
    "                \"compression_ratio\": n_comp / X.shape[1],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Testar com dígitos\n",
    "n_components_ml = [2, 5, 10, 15, 20, 30, 40, 50]\n",
    "results_df = compare_ml_with_pca(X_digits_scaled, y_digits, n_components_ml)\n",
    "\n",
    "print(\"Performance de Random Forest com PCA:\")\n",
    "print(\"=\" * 50)\n",
    "print(results_df.round(3))\n",
    "\n",
    "# Visualizar resultados\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Accuracy vs número de componentes\n",
    "numeric_results = results_df[results_df[\"n_components\"] != \"Original (64)\"]\n",
    "baseline_acc = results_df[results_df[\"n_components\"] == \"Original (64)\"][\"accuracy\"].iloc[0]\n",
    "\n",
    "axes[0].plot(\n",
    "    numeric_results[\"n_components\"], numeric_results[\"accuracy\"], \"bo-\", linewidth=2, markersize=8, label=\"Com PCA\"\n",
    ")\n",
    "axes[0].axhline(y=baseline_acc, color=\"red\", linestyle=\"--\", label=f\"Sem PCA ({baseline_acc:.3f})\")\n",
    "axes[0].set_xlabel(\"Número de Componentes PCA\")\n",
    "axes[0].set_ylabel(\"Accuracy\")\n",
    "axes[0].set_title(\"Performance vs Redução de Dimensionalidade\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].legend()\n",
    "\n",
    "# Trade-off: accuracy vs compressão\n",
    "axes[1].scatter(\n",
    "    numeric_results[\"compression_ratio\"],\n",
    "    numeric_results[\"accuracy\"],\n",
    "    s=100,\n",
    "    alpha=0.7,\n",
    "    c=numeric_results[\"n_components\"],\n",
    "    cmap=\"viridis\",\n",
    ")\n",
    "axes[1].axhline(y=baseline_acc, color=\"red\", linestyle=\"--\", alpha=0.7)\n",
    "axes[1].axvline(x=1.0, color=\"red\", linestyle=\"--\", alpha=0.7)\n",
    "axes[1].set_xlabel(\"Taxa de Compressão (n_comp / 64)\")\n",
    "axes[1].set_ylabel(\"Accuracy\")\n",
    "axes[1].set_title(\"Trade-off: Accuracy vs Compressão\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Adicionar colorbar\n",
    "colorbar = plt.colorbar(axes[1].collections[0], ax=axes[1])\n",
    "colorbar.set_label(\"Número de Componentes\")\n",
    "\n",
    "# Anotar pontos interessantes\n",
    "best_ratio_idx = numeric_results[\"accuracy\"].idxmax()\n",
    "best_comp = numeric_results.loc[best_ratio_idx, \"n_components\"]\n",
    "best_acc = numeric_results.loc[best_ratio_idx, \"accuracy\"]\n",
    "best_ratio = numeric_results.loc[best_ratio_idx, \"compression_ratio\"]\n",
    "\n",
    "axes[1].annotate(\n",
    "    f\"Melhor: {best_comp} comp.\\nAcc: {best_acc:.3f}\",\n",
    "    xy=(best_ratio, best_acc),\n",
    "    xytext=(best_ratio - 0.2, best_acc + 0.02),\n",
    "    arrowprops=dict(arrowstyle=\"->\", color=\"red\"),\n",
    "    bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.8),\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nMelhor configuração: {best_comp} componentes\")\n",
    "print(f\"Accuracy: {best_acc:.3f} vs {baseline_acc:.3f} (baseline)\")\n",
    "print(f\"Redução de features: {64} -> {best_comp} ({best_ratio:.1%})\")\n",
    "print(f\"Diferença de performance: {best_acc - baseline_acc:+.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0334f5ad",
   "metadata": {},
   "source": [
    "## 8. Interpretando Componentes Principais\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04db5150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar os primeiros componentes principais como imagens\n",
    "pca_interpretation = PCA(n_components=16)\n",
    "pca_interpretation.fit(X_digits_scaled)\n",
    "\n",
    "# Plotar componentes principais\n",
    "fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(16):\n",
    "    component_image = pca_interpretation.components_[i].reshape(8, 8)\n",
    "\n",
    "    # Plotar componente\n",
    "    im = axes[i].imshow(component_image, cmap=\"RdBu_r\")\n",
    "    axes[i].set_title(f\"PC{i+1}\\n({pca_interpretation.explained_variance_ratio_[i]:.1%} var)\")\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "    # Adicionar colorbar para alguns componentes\n",
    "    if i in [0, 1, 2, 3]:\n",
    "        plt.colorbar(im, ax=axes[i], fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.suptitle(\"Primeiros 16 Componentes Principais\\n(Padrões mais importantes nos dados)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analisar contribuição de cada pixel\n",
    "print(\"Interpretação dos Componentes Principais:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"PC1: Captura contraste geral (fundo vs dígito)\")\n",
    "print(\"PC2-PC4: Capturam formas e orientações específicas\")\n",
    "print(\"Componentes seguintes: Detalhes mais sutis e ruído\")\n",
    "\n",
    "# Mostrar como diferentes dígitos se projetam nos primeiros PCs\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for digit in range(10):\n",
    "    digit_mask = y_digits == digit\n",
    "    digit_projections = X_digits_2d[digit_mask]\n",
    "\n",
    "    axes[digit].scatter(digit_projections[:, 0], digit_projections[:, 1], alpha=0.6, s=20)\n",
    "    axes[digit].set_title(f\"Dígito {digit}\")\n",
    "    axes[digit].set_xlabel(\"PC1\")\n",
    "    axes[digit].set_ylabel(\"PC2\")\n",
    "    axes[digit].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Projeção de cada dígito nos primeiros 2 PCs\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5564ab7",
   "metadata": {},
   "source": [
    "## 9. Comparação com t-SNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a87ca36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar PCA com t-SNE para visualização\n",
    "# Usar subset dos dados para acelerar t-SNE\n",
    "n_samples_tsne = 500\n",
    "indices = np.random.choice(len(X_digits), n_samples_tsne, replace=False)\n",
    "X_subset = X_digits_scaled[indices]\n",
    "y_subset = y_digits[indices]\n",
    "\n",
    "# PCA 2D\n",
    "pca_viz = PCA(n_components=2)\n",
    "X_pca_viz = pca_viz.fit_transform(X_subset)\n",
    "\n",
    "# t-SNE 2D\n",
    "print(\"Aplicando t-SNE (pode demorar alguns segundos...)\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "X_tsne = tsne.fit_transform(X_subset)\n",
    "\n",
    "# Visualizar comparação\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# PCA\n",
    "scatter1 = axes[0].scatter(X_pca_viz[:, 0], X_pca_viz[:, 1], c=y_subset, cmap=\"tab10\", alpha=0.7, s=30)\n",
    "axes[0].set_xlabel(f\"PC1 ({pca_viz.explained_variance_ratio_[0]:.1%})\")\n",
    "axes[0].set_ylabel(f\"PC2 ({pca_viz.explained_variance_ratio_[1]:.1%})\")\n",
    "axes[0].set_title(f\"PCA 2D\\n(Variância total: {np.sum(pca_viz.explained_variance_ratio_):.1%})\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# t-SNE\n",
    "scatter2 = axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_subset, cmap=\"tab10\", alpha=0.7, s=30)\n",
    "axes[1].set_xlabel(\"t-SNE 1\")\n",
    "axes[1].set_ylabel(\"t-SNE 2\")\n",
    "axes[1].set_title(\"t-SNE 2D\\n(Preserva estrutura local)\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Adicionar colorbar\n",
    "plt.colorbar(scatter1, ax=axes[0], label=\"Dígito\")\n",
    "plt.colorbar(scatter2, ax=axes[1], label=\"Dígito\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calcular métricas de separação\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "pca_silhouette = silhouette_score(X_pca_viz, y_subset)\n",
    "tsne_silhouette = silhouette_score(X_tsne, y_subset)\n",
    "\n",
    "print(\"\\nComparação PCA vs t-SNE:\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"PCA Silhouette Score:  {pca_silhouette:.3f}\")\n",
    "print(f\"t-SNE Silhouette Score: {tsne_silhouette:.3f}\")\n",
    "print(\"\\nCaracterísticas:\")\n",
    "print(\"PCA:\")\n",
    "print(\"  + Rápido e determinístico\")\n",
    "print(\"  + Preserva variância global\")\n",
    "print(\"  + Permite reconstrução\")\n",
    "print(\"  - Assume relações lineares\")\n",
    "print(\"\\nt-SNE:\")\n",
    "print(\"  + Preserva estrutura local\")\n",
    "print(\"  + Melhor separação visual\")\n",
    "print(\"  - Lento para dados grandes\")\n",
    "print(\"  - Não determinístico\")\n",
    "print(\"  - Apenas para visualização\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a01adf",
   "metadata": {},
   "source": [
    "## 10. Resumo e Boas Práticas\n",
    "\n",
    "### Quando usar PCA:\n",
    "\n",
    "- **Redução de dimensionalidade** para acelerar algoritmos\n",
    "- **Visualização** de dados multidimensionais\n",
    "- **Compressão** com perda controlada\n",
    "- **Pré-processamento** para evitar curse of dimensionality\n",
    "- **Detecção de outliers** (pontos com alta reconstrução de erro)\n",
    "\n",
    "### Limitações do PCA:\n",
    "\n",
    "- Assume **relações lineares**\n",
    "- Sensível à **escala das features** (sempre normalizar)\n",
    "- Componentes podem ser **difíceis de interpretar**\n",
    "- Pode **remover informação importante** para classificação\n",
    "\n",
    "### Processo recomendado:\n",
    "\n",
    "1. **Normalizar dados** (StandardScaler)\n",
    "2. **Analisar variância explicada** para escolher número de componentes\n",
    "3. **Validar performance** de ML com diferentes números de componentes\n",
    "4. **Interpretar componentes** quando possível\n",
    "5. **Considerar alternativas** (t-SNE para visualização, kernel PCA para não-linearidade)\n",
    "\n",
    "### Regras práticas:\n",
    "\n",
    "- **85-95% da variância** geralmente suficiente\n",
    "- **Menos componentes que amostras** (n_components < n_samples)\n",
    "- **Testar com validação cruzada** para escolher número ótimo\n",
    "- **Aplicar PCA apenas no conjunto de treino**, depois transformar teste\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
